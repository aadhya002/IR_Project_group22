{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16HNqusWQtdy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1y3QnKdg0fG"
      },
      "outputs": [],
      "source": [
        "def cloth_styling(input_image_path, style_image_path, output_image_path):\n",
        "  %cd /content/drive/MyDrive/Interactive_cloth_segmentation_CV/cloth-segmentation\n",
        "  import os\n",
        "  # from tqdm import tqdm\n",
        "  from tqdm.notebook import tqdm\n",
        "  from PIL import Image\n",
        "  import numpy as np\n",
        "\n",
        "  import torch\n",
        "  import torch.nn.functional as F\n",
        "  import torchvision.transforms as transforms\n",
        "\n",
        "  from data.base_dataset import Normalize_image\n",
        "  from utils.saving_utils import load_checkpoint_mgpu\n",
        "\n",
        "  from networks import U2NET\n",
        "  device = 'cpu'\n",
        "\n",
        "  image_dir = '/content/drive/MyDrive/Interactive_cloth_segmentation_CV/input/'\n",
        "  result_dir = '/content/drive/MyDrive/Interactive_cloth_segmentation_CV/output/'\n",
        "  checkpoint_path = 'cloth_segm_u2net_latest.pth'\n",
        "\n",
        "  def get_palette(num_cls):\n",
        "      \"\"\" Returns the color map for visualizing the segmentation mask.\n",
        "      Args:\n",
        "          num_cls: Number of classes\n",
        "      Returns:\n",
        "          The color map\n",
        "      \"\"\"\n",
        "      n = num_cls\n",
        "      palette = [0] * (n * 3)\n",
        "      for j in range(0, n):\n",
        "          lab = j\n",
        "          palette[j * 3 + 0] = 0\n",
        "          palette[j * 3 + 1] = 0\n",
        "          palette[j * 3 + 2] = 0\n",
        "          i = 0\n",
        "          while lab:\n",
        "              palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n",
        "              palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n",
        "              palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n",
        "              i += 1\n",
        "              lab >>= 3\n",
        "      return palette\n",
        "\n",
        "\n",
        "  transforms_list = []\n",
        "  transforms_list += [transforms.ToTensor()]\n",
        "  transforms_list += [Normalize_image(0.5, 0.5)]\n",
        "  transform_rgb = transforms.Compose(transforms_list)\n",
        "\n",
        "  net = U2NET(in_ch=3, out_ch=4)\n",
        "  net = load_checkpoint_mgpu(net, checkpoint_path)\n",
        "  net = net.to(device)\n",
        "  net = net.eval()\n",
        "\n",
        "  palette = get_palette(4)\n",
        "\n",
        "  !rm -rf '/content/drive/MyDrive/Interactive_cloth_segmentation_CV/input/.ipynb_checkpoints'\n",
        "  images_list = sorted(os.listdir(image_dir))\n",
        "  print(images_list)\n",
        "  pbar = tqdm(total=len(images_list))\n",
        "  def getSegmented_image():\n",
        "    ret = []\n",
        "    for image_name in images_list:\n",
        "        img = Image.open(os.path.join(image_dir, image_name)).convert('RGB')\n",
        "        img_size = img.size\n",
        "        img = img.resize((768, 768), Image.BICUBIC)\n",
        "        image_tensor = transform_rgb(img)\n",
        "        image_tensor = torch.unsqueeze(image_tensor, 0)\n",
        "        # A X 1 X B X 1 X C X 1 = A X B X C\n",
        "        output_tensor = net(image_tensor.to(device))\n",
        "        # print(output_tensor)\n",
        "        # output_tensor = F.log_softmax(output_tensor[0], dim=1)\n",
        "        print(output_tensor[0].shape,\"1\")\n",
        "        output_tensor = torch.max(output_tensor[0], dim=1, keepdim=True)[1]\n",
        "        print(output_tensor.shape,\"2\")\n",
        "        output_tensor = torch.squeeze(output_tensor, dim=0)\n",
        "        print(output_tensor.shape,\"3\")\n",
        "        output_tensor = torch.squeeze(output_tensor, dim=0)\n",
        "        print(output_tensor.shape,\"4\")\n",
        "        output_arr = output_tensor.cpu().numpy()\n",
        "\n",
        "        output_img = Image.fromarray(output_arr.astype('uint8'), mode='L')\n",
        "        output_img = output_img.resize(img_size, Image.BICUBIC)\n",
        "        output_img_before = Image.fromarray(output_arr.astype('uint8')*255, mode='L')\n",
        "        # output_img_before.save(os.path.join(result_dir, image_name[:-4]+'_before.png'))\n",
        "\n",
        "        output_img.putpalette(palette)\n",
        "        output_img.save(os.path.join(result_dir, image_name[:-4]+'_segmented.png'))\n",
        "        ret.append(output_img)\n",
        "        print(image_name)\n",
        "        pbar.update(1)\n",
        "    return ret\n",
        "  # getSegmented_image()\n",
        "  pbar.close()\n",
        "\n",
        "  import os\n",
        "  import tensorflow as tf\n",
        "  # Load compressed models from tensorflow_hub\n",
        "  os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
        "  import IPython.display as display\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  import matplotlib as mpl\n",
        "  mpl.rcParams['figure.figsize'] = (12, 12)\n",
        "  mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "  import numpy as np\n",
        "  import PIL.Image\n",
        "  import time\n",
        "  import functools\n",
        "\n",
        "  def tensor_to_image(tensor):\n",
        "    tensor = tensor*255\n",
        "    tensor = np.array(tensor, dtype=np.uint8)\n",
        "    if np.ndim(tensor)>3:\n",
        "      assert tensor.shape[0] == 1\n",
        "      tensor = tensor[0]\n",
        "    return PIL.Image.fromarray(tensor)\n",
        "\n",
        "  def load_img(path_to_img):\n",
        "    max_dim = 512\n",
        "    img = tf.io.read_file(path_to_img)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "    long_dim = max(shape)\n",
        "    scale = max_dim / long_dim\n",
        "\n",
        "    new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "    img = tf.image.resize(img, new_shape)\n",
        "    img = img[tf.newaxis, :]\n",
        "    return img\n",
        "\n",
        "  def imshow(image, title=None):\n",
        "    if len(image.shape or image.size) > 3:\n",
        "      image = tf.squeeze(image, axis=0)\n",
        "\n",
        "    plt.imshow(image)\n",
        "    if title:\n",
        "      plt.title(title)\n",
        "  def imshow_pil(image, title=None):\n",
        "    if len(image.size) > 3:\n",
        "      image = tf.squeeze(image, axis=0)\n",
        "\n",
        "    plt.imshow(image)\n",
        "    if title:\n",
        "      plt.title(title)\n",
        "  \n",
        "  import tensorflow_hub as hub\n",
        "  hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "  \n",
        "  content_image = load_img(input_image_path)\n",
        "  style_image = load_img(style_image_path)\n",
        "  stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
        "  stylized_image = tensor_to_image(stylized_image)\n",
        "\n",
        "  %cd /content/drive/MyDrive/Interactive_cloth_segmentation_CV/U-2-Net\n",
        "  # U-2-Net -> Saliency\n",
        "  # U-2-Net only segmentation -> Segmentation\n",
        "\n",
        "  import os\n",
        "  from skimage import io, transform\n",
        "  import torch\n",
        "  import torchvision\n",
        "  from torch.autograd import Variable\n",
        "  import torch.nn as nn\n",
        "  import torch.nn.functional as F\n",
        "  from torch.utils.data import Dataset, DataLoader\n",
        "  from torchvision import transforms#, utils\n",
        "  # import torch.optim as optim\n",
        "\n",
        "  import numpy as np\n",
        "  from PIL import Image\n",
        "  import glob\n",
        "\n",
        "  from data_loader import RescaleT\n",
        "  from data_loader import ToTensor\n",
        "  from data_loader import ToTensorLab\n",
        "  from data_loader import SalObjDataset\n",
        "\n",
        "  from model import U2NET # full size version 173.6 MB\n",
        "  from model import U2NETP # small version u2net 4.7 MB\n",
        "\n",
        "  # normalize the predicted SOD probability map\n",
        "  def normPRED(d):\n",
        "      ma = torch.max(d)\n",
        "      mi = torch.min(d)\n",
        "\n",
        "      dn = (d-mi)/(ma-mi)\n",
        "\n",
        "      return dn\n",
        "\n",
        "  def save_output(image_name,pred,d_dir):\n",
        "\n",
        "      predict = pred\n",
        "      predict = predict.squeeze()\n",
        "      predict_np = predict.cpu().data.numpy()\n",
        "\n",
        "      im = Image.fromarray(predict_np*255).convert('RGB')\n",
        "      img_name = image_name.split(os.sep)[-1]\n",
        "      image = io.imread(image_name)\n",
        "      imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n",
        "\n",
        "      pb_np = np.array(imo)\n",
        "\n",
        "      aaa = img_name.split(\".\")\n",
        "      bbb = aaa[0:-1]\n",
        "      imidx = bbb[0]\n",
        "      for i in range(1,len(bbb)):\n",
        "          imidx = imidx + \".\" + bbb[i]\n",
        "\n",
        "      imo.save(d_dir+imidx+'_saliency.png')\n",
        "      return imo\n",
        "\n",
        "  def get_saliency_maps_name(image_name):\n",
        "\n",
        "      # --------- 1. get image path and name ---------\n",
        "      model_name='u2net'#u2netp\n",
        "\n",
        "\n",
        "\n",
        "      image_dir =  '../input'\n",
        "      prediction_dir =  '../output/'\n",
        "      model_dir = os.path.join(os.getcwd(), 'saved_models', model_name, model_name + '.pth')\n",
        "      print(os.getcwd())\n",
        "      img_name_list = glob.glob(image_dir + os.sep + '*')\n",
        "      print(img_name_list)\n",
        "\n",
        "      # --------- 2. dataloader ---------\n",
        "      #1. dataloader\n",
        "      test_salobj_dataset = SalObjDataset(img_name_list = img_name_list,\n",
        "                                          lbl_name_list = [],\n",
        "                                          transform=transforms.Compose([RescaleT(320),\n",
        "                                                                        ToTensorLab(flag=0)])\n",
        "                                          )\n",
        "      test_salobj_dataloader = DataLoader(test_salobj_dataset,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=1)\n",
        "\n",
        "      # --------- 3. model define ---------\n",
        "      if(model_name=='u2net'):\n",
        "          print(\"...load U2NET---173.6 MB\")\n",
        "          net = U2NET(3,1)\n",
        "      elif(model_name=='u2netp'):\n",
        "          print(\"...load U2NEP---4.7 MB\")\n",
        "          net = U2NETP(3,1)\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          net.load_state_dict(torch.load(model_dir))\n",
        "          net.cuda()\n",
        "      else:\n",
        "          net.load_state_dict(torch.load(model_dir, map_location='cpu'))\n",
        "      net.eval()\n",
        "\n",
        "      # --------- 4. inference for each image ---------\n",
        "      ret = []\n",
        "      for i_test, data_test in enumerate(test_salobj_dataloader):\n",
        "          if(img_name_list[i_test].split(os.sep)[-1]==image_name):\n",
        "\n",
        "            print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n",
        "\n",
        "            inputs_test = data_test['image']\n",
        "            inputs_test = inputs_test.type(torch.FloatTensor)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                inputs_test = Variable(inputs_test.cuda())\n",
        "            else:\n",
        "                inputs_test = Variable(inputs_test)\n",
        "\n",
        "            d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n",
        "\n",
        "            # normalization\n",
        "            pred = d1[:,0,:,:]\n",
        "            pred = normPRED(pred)\n",
        "\n",
        "            # save results to test_results folder\n",
        "            if not os.path.exists(prediction_dir):\n",
        "                os.makedirs(prediction_dir, exist_ok=True)\n",
        "            img = save_output(img_name_list[i_test],pred,prediction_dir)\n",
        "            ret.append(img)\n",
        "            del d1,d2,d3,d4,d5,d6,d7\n",
        "      return ret\n",
        "  def get_saliency_maps():\n",
        "\n",
        "      # --------- 1. get image path and name ---------\n",
        "      model_name='u2net'#u2netp\n",
        "\n",
        "\n",
        "\n",
        "      image_dir =  '../input'\n",
        "      prediction_dir =  '../output/'\n",
        "      model_dir = os.path.join(os.getcwd(), 'saved_models', model_name, model_name + '.pth')\n",
        "      print(os.getcwd())\n",
        "      img_name_list = glob.glob(image_dir + os.sep + '*')\n",
        "      print(img_name_list)\n",
        "\n",
        "      # --------- 2. dataloader ---------\n",
        "      #1. dataloader\n",
        "      test_salobj_dataset = SalObjDataset(img_name_list = img_name_list,\n",
        "                                          lbl_name_list = [],\n",
        "                                          transform=transforms.Compose([RescaleT(320),\n",
        "                                                                        ToTensorLab(flag=0)])\n",
        "                                          )\n",
        "      test_salobj_dataloader = DataLoader(test_salobj_dataset,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=1)\n",
        "\n",
        "      # --------- 3. model define ---------\n",
        "      if(model_name=='u2net'):\n",
        "          print(\"...load U2NET---173.6 MB\")\n",
        "          net = U2NET(3,1)\n",
        "      elif(model_name=='u2netp'):\n",
        "          print(\"...load U2NEP---4.7 MB\")\n",
        "          net = U2NETP(3,1)\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          net.load_state_dict(torch.load(model_dir))\n",
        "          net.cuda()\n",
        "      else:\n",
        "          net.load_state_dict(torch.load(model_dir, map_location='cpu'))\n",
        "      net.eval()\n",
        "\n",
        "      # --------- 4. inference for each image ---------\n",
        "      ret = []\n",
        "      for i_test, data_test in enumerate(test_salobj_dataloader):\n",
        "          \n",
        "\n",
        "          print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n",
        "\n",
        "          inputs_test = data_test['image']\n",
        "          inputs_test = inputs_test.type(torch.FloatTensor)\n",
        "\n",
        "          if torch.cuda.is_available():\n",
        "              inputs_test = Variable(inputs_test.cuda())\n",
        "          else:\n",
        "              inputs_test = Variable(inputs_test)\n",
        "\n",
        "          d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n",
        "\n",
        "          # normalization\n",
        "          pred = d1[:,0,:,:]\n",
        "          pred = normPRED(pred)\n",
        "\n",
        "          # save results to test_results folder\n",
        "          if not os.path.exists(prediction_dir):\n",
        "              os.makedirs(prediction_dir, exist_ok=True)\n",
        "          img = save_output(img_name_list[i_test],pred,prediction_dir)\n",
        "          ret.append(img)\n",
        "          del d1,d2,d3,d4,d5,d6,d7\n",
        "      return ret\n",
        "\n",
        "  %cd /content/drive/MyDrive/Interactive_cloth_segmentation_CV/cloth-segmentation\n",
        "  segmented_images = getSegmented_image()\n",
        "  len(segmented_images)\n",
        "  %cd /content/drive/MyDrive/Interactive_cloth_segmentation_CV/U-2-Net\n",
        "  saliency_maps = get_saliency_maps()\n",
        "\n",
        "  # print(saliency_maps[0].size, stylized_image.size, segmented_images[0].size)\n",
        "  #@title ## Input Image Paths\n",
        "  stylized_images = []\n",
        "\n",
        "  style_image = load_img(style_image_path)\n",
        "\n",
        "  content_Image_path = \"/content/drive/MyDrive/Interactive_cloth_segmentation_CV/input/\"\n",
        "  reuse_old_image = False\n",
        "  for i in range(len(saliency_maps)):\n",
        "    content = load_img(content_Image_path+images_list[i])\n",
        "    stylized = hub_model(tf.constant(content), tf.constant(style_image))[0]\n",
        "    stylized = tensor_to_image(stylized)\n",
        "    stylized_images.append(stylized)\n",
        "    saliency_maps[i] = saliency_maps[i].resize((stylized.size[0],stylized.size[1]),Image.BICUBIC)\n",
        "    segmented_images[i] = segmented_images[i].resize((stylized.size[0],stylized.size[1]),Image.BICUBIC)\n",
        "  if(not(reuse_old_image)):\n",
        "    inputs_img = []\n",
        "    inputs_img2 = []\n",
        "    for i in range(len(images_list)):\n",
        "      img = Image.open(os.path.join(image_dir, images_list[i])).convert('RGB')\n",
        "      img2 = Image.open(os.path.join(image_dir, images_list[i])).convert('RGB')\n",
        "      img = img.resize((stylized_images[i].size[0],stylized_images[i].size[1]),Image.BICUBIC)\n",
        "      inputs_img.append(img)\n",
        "      inputs_img2.append(img2)\n",
        "  for i in range(len(segmented_images)):\n",
        "    segmented_images[i] = segmented_images[i].convert('RGB')\n",
        "\n",
        "  \n",
        "  plt.subplot(1, 3, 1)\n",
        "  imshow_pil(saliency_maps[0],\"saliency maps\")\n",
        "  plt.subplot(1, 3, 2)\n",
        "  imshow_pil(stylized_images[0],\"stylized image\")\n",
        "  plt.subplot(1, 3, 3)\n",
        "  imshow_pil(segmented_images[0],\"segmented image\")\n",
        "\n",
        "  # plt.subplot(1, 3, 1)\n",
        "  from matplotlib.patches import Rectangle\n",
        "  from matplotlib.gridspec import GridSpec\n",
        "  na = np.array(segmented_images[0].convert('RGB'))\n",
        "  colours, counts = np.unique(na.reshape(-1,3), axis=0, return_counts=1)\n",
        "  # print(colours)\n",
        "  gs = GridSpec(6,1)\n",
        "\n",
        "  fig = plt.figure(figsize = (10,10))\n",
        "  ax1 = fig.add_subplot(gs[:-1,:]) ##for the plot\n",
        "  ax2 = fig.add_subplot(gs[-1,:]) \n",
        "  legend_data = []\n",
        "  # print(colours)\n",
        "  ax1.imshow(segmented_images[0])\n",
        "  for i in range(len(colours)):\n",
        "    if(colours[i][0]==0 and colours[i][1]==0 and colours[i][2]==0):\n",
        "      continue\n",
        "    temp = [0,colours[i],i]\n",
        "    legend_data.append(temp)\n",
        "\n",
        "  handles = [\n",
        "      Rectangle((0,0),1,1, color = tuple((v/255 for v in c))) for k,c,n in legend_data\n",
        "  ]\n",
        "  labels = [n for k,c,n in legend_data]\n",
        "  ax2.legend(handles,labels, mode='expand', ncol=3)\n",
        "  ax2.axis('off')\n",
        "\n",
        "  plt.legend(handles,labels)\n",
        "  plt.show()\n",
        "\n",
        "  ctr =0\n",
        "  chosen_segment =  2\n",
        "  segmented_styled = []\n",
        "  for i in range(len(segmented_images)):\n",
        "    newImage = segmented_images[i].copy()\n",
        "    pix_seg = segmented_images[i].load()\n",
        "    pix_sty = stylized_images[i].load()\n",
        "    \n",
        "    pix_seg_sty = newImage.load()\n",
        "    for x in range((segmented_images[i].size[0])):\n",
        "      for y in range((segmented_images[i].size[1])):\n",
        "        if(pix_seg[x,y]==(colours[chosen_segment][0],colours[chosen_segment][1],colours[chosen_segment][2])):\n",
        "          # print(pix_sty[x,y])\n",
        "          # print((pix_seg[x,y]))\n",
        "          pix_seg_sty[x,y] = pix_sty[x,y]\n",
        "\n",
        "          # break\n",
        "    segmented_styled.append(newImage)\n",
        "\n",
        "  fig = plt.figure(figsize = (10,10))\n",
        "  ax1 = fig.add_subplot(gs[:-1,:]) ##for the plot\n",
        "  ax2 = fig.add_subplot(gs[-1,:]) \n",
        "  legend_data = []\n",
        "  # print(colours)\n",
        "  ax1.imshow(segmented_styled[0])\n",
        "  for i in range(len(colours)):\n",
        "    if(colours[i][0]==0 and colours[i][1]==0 and colours[i][2]==0):\n",
        "      continue\n",
        "    temp = [0,colours[i],i]\n",
        "    legend_data.append(temp)\n",
        "\n",
        "  handles = [\n",
        "      Rectangle((0,0),1,1, color = tuple((v/255 for v in c))) for k,c,n in legend_data\n",
        "  ]\n",
        "  labels = [n for k,c,n in legend_data]\n",
        "  ax2.legend(handles,labels, mode='expand', ncol=3)\n",
        "  ax2.axis('off')\n",
        "\n",
        "  plt.legend(handles,labels)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  import colorsys\n",
        "\n",
        "  res = []\n",
        "  import copy\n",
        "  for i in range(len(segmented_images)):\n",
        "    pix_unstyled = segmented_images[i].load()\n",
        "    pix_seg = segmented_styled[i].load()\n",
        "    pix_saliency = saliency_maps[i].convert(mode='L').load()\n",
        "    pix_sty = stylized_images[i].load()\n",
        "    pix_inp = inputs_img[i].load()\n",
        "    newImage = copy.deepcopy(inputs_img[i])\n",
        "    lum_image = copy.deepcopy(inputs_img[i])\n",
        "    pix_lum = lum_image.convert(mode=\"L\").load()\n",
        "    # environment = brightness(inputs_img[i])\n",
        "    pix_nimg = newImage.load()\n",
        "    for x in range((segmented_images[i].size[0])):\n",
        "      for y in range((segmented_images[i].size[1])):\n",
        "        if(pix_unstyled[x,y]==(colours[chosen_segment][0],colours[chosen_segment][1],colours[chosen_segment][2])):\n",
        "          temp = colorsys.rgb_to_hsv(pix_seg[x,y][0]/255, pix_seg[x,y][1]/255, pix_seg[x,y][2]/255)\n",
        "          temp2 = colorsys.rgb_to_hsv(pix_inp[x,y][0]/255, pix_inp[x,y][1]/255, pix_inp[x,y][2]/255)\n",
        "          # print(temp2[2])\n",
        "          final = colorsys.hsv_to_rgb(temp[0], temp[1], temp2[2])\n",
        "          # print(pix_saliency[x,y])\n",
        "          pix_nimg[x,y] = tuple(int(i) for i in pix_seg[x,y])\n",
        "          if(pix_saliency[x,y]==255):\n",
        "            pix_nimg[x,y] = tuple(int(i) for i in pix_seg[x,y])\n",
        "          else:\n",
        "            saliency_binary_val = (pix_saliency[x,y]/255)\n",
        "            final = tuple((saliency_binary_val*pix_seg[x,y][i] + (1-saliency_binary_val)*pix_inp[x,y][i]) for i in range(3))\n",
        "            # print(final)\n",
        "          # print(pix_seg[x,y], final)                           \n",
        "            pix_inp[x,y] = tuple(int(i) for i in final)\n",
        "          \n",
        "          # print(pix_nimg[x,y])\n",
        "  # newImage = newImage.convert('RGB')\n",
        "  res.append(newImage)\n",
        "  newImage.save(output_image_path)\n",
        "  return newImage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUDr0J3dnIbV"
      },
      "outputs": [],
      "source": [
        "def pose_transfer(input_image_path,person):\n",
        "  from PIL import Image\n",
        "  %cd /content\n",
        "  !git clone https://github.com/PeikeLi/Self-Correction-Human-Parsing\n",
        "  %cd Self-Correction-Human-Parsing\n",
        "  !mkdir checkpoints\n",
        "  !mkdir inputs\n",
        "  !mkdir outputs\n",
        "  dataset = 'lip'         #select from ['lip', 'atr', 'pascal']\n",
        "  !gdown --id 1h0QEmZQleNCWBY-Fvv7yrRyRT_kDZHez -O checkpoints/final.pth\n",
        "  !pip install ninja\n",
        "  !pip install tensorboardX\n",
        "  %cd /content/Self-Correction-Human-Parsing\n",
        "  !python3 simple_extractor.py --dataset 'lip' --model-restore 'checkpoints/final.pth' --input-dir 'inputs' --output-dir 'outputs'\n",
        "  %cd ..\n",
        "  from pathlib import Path\n",
        "  import os\n",
        "\n",
        "  if(person == \"man\"):\n",
        "    my_file1 = Path(\"/content/drive/MyDrive/dressing-in-order/DATA_ROOT/test/fashionMENShortsid0000098901_1front.jpg\")\n",
        "    if my_file1.is_file():\n",
        "      os.remove(\"/content/drive/MyDrive/dressing-in-order/DATA_ROOT/test/fashionMENShortsid0000098901_1front.jpg\")\n",
        "      print(\"removed from test\")\n",
        "\n",
        "    uploaded1 = Image.open(input_image_path) \n",
        "    uploaded1 = uploaded1.save(\"/content/drive/MyDrive/dressing-in-order/DATA_ROOT/test/fashionMENShortsid0000098901_1front.jpg\")\n",
        "\n",
        "  elif(person == \"women\"):\n",
        "    my_file1 = Path(\"/content/drive/MyDrive/dressing-in-order/DATA_ROOT/test/fashionWOMENTees_Tanksid0000336104_1front.jpg\")\n",
        "    if my_file1.is_file():\n",
        "      os.remove(\"/content/drive/MyDrive/dressing-in-order/DATA_ROOT/test/fashionWOMENTees_Tanksid0000336104_1front.jpg\")\n",
        "      print(\"removed from test\")\n",
        "\n",
        "    uploaded1 = Image.open(input_image_path) \n",
        "    uploaded1 = uploaded1.save(\"/content/drive/MyDrive/dressing-in-order/DATA_ROOT/test/fashionWOMENTees_Tanksid0000336104_1front.jpg\")\n",
        "\n",
        "\n",
        "  !gdown --id 1SkCnqhGIHOSnlRQL3GkjYfj899cOWYTF -O checkpoints.zip\n",
        "  !gdown --id 1pRIwjWAos9q5or04fnK44SPukSI616bS -O datasets.zip\n",
        "  !gdown --id 1bTFAC6cqdV57FbbiUfSPtnLJG0AI-jFK -O Images.zip\n",
        "  !gdown --id 1p2gAcET7JTuHRCsXMDRf4-Pwftk1_rQW -O models.zip\n",
        "  !gdown --id 1FeLBMHp2l4CZrV0Nvr8eIXl4Wm6dKVBm -O options.zip\n",
        "  !gdown --id 1PAY_1DQ6LiY6_fxbz8m4U7dxEIftd1i1 -O scripts.zip\n",
        "  !gdown --id 1H6baRQN5naV22e5e5fgeD0moObAZ4BR3 -O tools.zip\n",
        "  !gdown --id 1wm_uKVE1S4a7nlwZfGK6pbTyRPSBD6AZ -O utils.zip\n",
        "  !gdown --id 1t6SN1hJvfp-zzDsn8v7LE5snImu5i1kw -O pretrained_models.zip\n",
        "\n",
        "  ! unzip checkpoints.zip \n",
        "  ! unzip datasets.zip \n",
        "  ! unzip Images.zip \n",
        "  ! unzip models.zip \n",
        "  ! unzip options.zip \n",
        "  ! unzip scripts.zip \n",
        "  ! unzip tools.zip \n",
        "  ! unzip utils.zip -d /content/utils\n",
        "  ! unzip pretrained_models.zip -d /content/pretrained_models\n",
        "\n",
        "  import sys\n",
        "  sys.path.append('../')\n",
        "  import torch\n",
        "  from models.dior_model import DIORModel\n",
        "  import os, json\n",
        "  import torch.nn.functional as F\n",
        "  from PIL import Image\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "\n",
        "\n",
        "  dataroot = '/content/drive/MyDrive/dressing-in-order/DATA_ROOT'\n",
        "  exp_name = 'DIOR_64' # DIORv1_64\n",
        "  epoch = 'latest'\n",
        "  netG = 'dior' # diorv1\n",
        "  ngf = 64\n",
        "\n",
        "  ## this is a dummy \"argparse\" \n",
        "  class Opt:\n",
        "      def __init__(self):\n",
        "          pass\n",
        "  if True:\n",
        "      opt = Opt()\n",
        "      opt.dataroot = dataroot\n",
        "      opt.isTrain = False\n",
        "      opt.phase = 'test'\n",
        "      opt.n_human_parts = 8; opt.n_kpts = 18; opt.style_nc = 64\n",
        "      opt.n_style_blocks = 4; opt.netG = netG; opt.netE = 'adgan'\n",
        "      opt.ngf = ngf\n",
        "      opt.norm_type = 'instance'; opt.relu_type = 'leakyrelu'\n",
        "      opt.init_type = 'orthogonal'; opt.init_gain = 0.02; opt.gpu_ids = [0]\n",
        "      opt.frozen_flownet = True; opt.random_rate = 1; opt.perturb = False; opt.warmup=False\n",
        "      opt.name = exp_name\n",
        "      opt.vgg_path = ''; opt.flownet_path = 'pretrained_models/flownet.pt'\n",
        "      opt.checkpoints_dir = 'checkpoints'\n",
        "      opt.frozen_enc = True\n",
        "      opt.load_iter = 0\n",
        "      opt.epoch = epoch\n",
        "      opt.verbose = False\n",
        "\n",
        "  # create model\n",
        "  model = DIORModel(opt)\n",
        "  model.setup(opt)\n",
        "\n",
        "  # load data\n",
        "  from datasets.deepfashion_datasets import DFVisualDataset\n",
        "  Dataset = DFVisualDataset\n",
        "  ds = Dataset(dataroot=dataroot, dim=(256,176), n_human_part=8)\n",
        "\n",
        "  # preload a set of pre-selected models defined in \"standard_test_anns.txt\" for quick visualizations \n",
        "  inputs = dict()\n",
        "  for attr in ds.attr_keys:\n",
        "      inputs[attr] = ds.get_attr_visual_input(attr)\n",
        "      \n",
        "  # define some tool functions for I/O\n",
        "  def load_img(pid, ds):\n",
        "      if isinstance(pid,str): # load pose from scratch\n",
        "          return None, None, load_pose_from_json(pid)\n",
        "      if len(pid[0]) < 10: # load pre-selected models\n",
        "          person = inputs[pid[0]]\n",
        "          person = (i.cuda() for i in person)\n",
        "          pimg, parse, to_pose = person\n",
        "          pimg, parse, to_pose = pimg[pid[1]], parse[pid[1]], to_pose[pid[1]]\n",
        "      else: # load model from scratch\n",
        "          person = ds.get_inputs_by_key(pid[0])\n",
        "          person = (i.cuda() for i in person)\n",
        "          pimg, parse, to_pose = person\n",
        "      return pimg.squeeze(), parse.squeeze(), to_pose.squeeze()\n",
        "\n",
        "  def load_pose_from_json(ani_pose_dir):\n",
        "      with open(ani_pose_dir, 'r') as f:\n",
        "          anno = json.load(f)\n",
        "      len(anno['people'][0]['pose_keypoints_2d'])\n",
        "      anno = list(anno['people'][0]['pose_keypoints_2d'])\n",
        "      x = np.array(anno[1::3])\n",
        "      y = np.array(anno[::3])\n",
        "\n",
        "      coord = np.concatenate([x[:,None], y[:,None]], -1)\n",
        "      #import pdb; pdb.set_trace()\n",
        "      #coord = (coord * 1.1) - np.array([10,30])[None, :]\n",
        "      pose  = pose_utils.cords_to_map(coord, (256,176), (256, 256))\n",
        "      pose = np.transpose(pose,(2, 0, 1))\n",
        "      pose = torch.Tensor(pose)\n",
        "      return pose\n",
        "\n",
        "  def plot_img(pimg=[], gimgs=[], oimgs=[], gen_img=[], pose=None,folder='',number=''):\n",
        "      if pose != None:\n",
        "          import utils.pose_utils as pose_utils\n",
        "          print(pose.size())\n",
        "          kpt = pose_utils.draw_pose_from_map(pose.cpu().numpy().transpose(1,2,0),radius=6)\n",
        "          kpt = kpt[0]\n",
        "      if not isinstance(pimg, list):\n",
        "          pimg = [pimg]\n",
        "      if not isinstance(gen_img, list):\n",
        "          gen_img = [gen_img]\n",
        "      out = pimg + gimgs + oimgs + gen_img\n",
        "      if out:\n",
        "          out = torch.cat(out, 2).float().cpu().detach().numpy()\n",
        "          out = (out + 1) / 2 # denormalize\n",
        "          out = np.transpose(out, [1,2,0])\n",
        "\n",
        "          if pose != None:\n",
        "              out = np.concatenate((kpt, out),1)\n",
        "      else:\n",
        "          out = kpt\n",
        "      fig = plt.figure(figsize=(6,4), dpi= 100, facecolor='w', edgecolor='k')\n",
        "      plt.axis('off')\n",
        "      plt.imshow(out)\n",
        "      result_dir=\"Results/\"+folder+\"/\"+\"Pose\"+number\n",
        "      fig.savefig(result_dir)\n",
        "\n",
        "  # define dressing-in-order function (the pipeline)\n",
        "  def dress_in_order(model, pid, pose_id=None, gids=[], ogids=[], order=[5,1,3,2], perturb=False):\n",
        "      PID = [0,4,6,7]\n",
        "      GID = [2,5,1,3]\n",
        "      # encode person\n",
        "      pimg, parse, from_pose = load_img(pid, ds)\n",
        "      if perturb:\n",
        "          pimg = perturb_images(pimg[None])[0]\n",
        "      if not pose_id:\n",
        "          to_pose = from_pose\n",
        "      else:\n",
        "          to_img, _, to_pose = load_img(pose_id, ds)\n",
        "      psegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None], PID)\n",
        "\n",
        "      # encode base garments\n",
        "      gsegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None])\n",
        "    \n",
        "      \n",
        "      # swap base garment if any\n",
        "      gimgs = []\n",
        "      for gid in gids:\n",
        "          _,_,k = gid\n",
        "          gimg, gparse, pose =  load_img(gid, ds)\n",
        "          seg = model.encode_single_attr(gimg[None], gparse[None], pose[None], to_pose[None], i=gid[2])\n",
        "          gsegs[gid[2]] = seg\n",
        "          gimgs += [gimg * (gparse == gid[2])]\n",
        "\n",
        "      # encode garment (overlay)\n",
        "      garments = []\n",
        "      over_gsegs = []\n",
        "      oimgs = []\n",
        "      for gid in ogids:\n",
        "          oimg, oparse, pose = load_img(gid, ds)\n",
        "          oimgs += [oimg * (oparse == gid[2])]\n",
        "          seg = model.encode_single_attr(oimg[None], oparse[None], pose[None], to_pose[None], i=gid[2])\n",
        "          over_gsegs += [seg]\n",
        "      \n",
        "      gsegs = [gsegs[i] for i in order] + over_gsegs\n",
        "      gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
        "      \n",
        "      return pimg, gimgs, oimgs, gen_img[0], to_pose\n",
        "\n",
        "  !mkdir Results\n",
        "  !mkdir Results/Pose\n",
        "\n",
        "  import numpy as np\n",
        "  %matplotlib notebook\n",
        "  %matplotlib inline\n",
        "  # person id\n",
        "  if person == \"men\":\n",
        "    pid = ('plain', 1, None)\n",
        "  elif person == \"women\":\n",
        "    pid = ('plain', 3, None)\n",
        "  # pid = (\"lace\", 0, None) \n",
        "  # pid= (\"lace\" ,1 , None)\n",
        "  # pid= (\"print\" ,0 , None)\n",
        "  for i in range(0,4):\n",
        "    pose_id = (\"lace\", i, None) \n",
        "    pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "    plot_img(pimg, gimgs, oimgs, gen_img, pose,\"Pose\",str(i+1))\n",
        "\n",
        "  for i in range(0,4):\n",
        "    pose_id = (\"plaid\", i, None) \n",
        "    pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "    plot_img(pimg, gimgs, oimgs, gen_img, pose,\"Pose\",str(i+5))\n",
        "\n",
        "  # pid= (\"plaid\" ,0, None)\n",
        "\n",
        "  for i in range(0,4):\n",
        "    pose_id = (\"plain\", i, None) \n",
        "    pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "    plot_img(pimg, gimgs, oimgs, gen_img, pose,\"Pose\",str(i+9))\n",
        "\n",
        "  for i in range(0,4):\n",
        "    pose_id = (\"pattern\", i, None) \n",
        "    pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "    plot_img(pimg, gimgs, oimgs, gen_img, pose,\"Pose\",str(i+13))\n",
        "\n",
        "  for i in range(0,4):\n",
        "    pose_id = (\"strip\", i, None) \n",
        "    pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "    plot_img(pimg, gimgs, oimgs, gen_img, pose,\"Pose\",str(i+17))\n",
        "\n",
        "  # pid= (\"gfla\" ,1 , None)\n",
        "\n",
        "  for i in range(0,4):\n",
        "    pose_id = (\"print\", i, None) \n",
        "    pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "    plot_img(pimg, gimgs, oimgs, gen_img, pose,\"Pose\",str(i+21))\n",
        "\n",
        "  # pid= (\"plain\" ,2 , None)\n",
        "\n",
        "  for i in range(0,4):\n",
        "    pose_id = (\"collar\", i, None) \n",
        "    pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "    plot_img(pimg, gimgs, oimgs, gen_img, pose,\"Pose\",str(i+25))\n",
        "\n",
        "\n",
        "  for i in range(0,4):\n",
        "    pose_id = (\"gfla\", i, None) \n",
        "    pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "    plot_img(pimg, gimgs, oimgs, gen_img, pose,\"Pose\",str(i+29))\n",
        "\n",
        "  # pid= (\"pattern\" ,1 , None)\n",
        "\n",
        "  for i in range(0,4):\n",
        "    pose_id = (\"flower\", i, None) \n",
        "    pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "    plot_img(pimg, gimgs, oimgs, gen_img, pose,\"Pose\",str(i+33))\n",
        "\n",
        "  for i in range(0,4):\n",
        "    pose_id = (\"jacket\", i, None) \n",
        "    pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "    plot_img(pimg, gimgs, oimgs, gen_img, pose,\"Pose\",str(i+37))\n",
        "\n",
        "\n",
        "  # !zip -r /content/results.zip /content/Results\n",
        "  # from google.colab import files\n",
        "  # files.download(\"/content/results.zip\")\n",
        "  # files.download(input_image_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXplKZNRq2PD"
      },
      "outputs": [],
      "source": [
        "def run_final(input_image_path, style_image_path, output_image_path,person):\n",
        "  cloth_styling(input_image_path, style_image_path, output_image_path)\n",
        "  pose_transfer(output_image_path,person)\n",
        "\n",
        "style_image = \"/content/drive/MyDrive/Interactive_cloth_segmentation_CV/style/style_image.jpg\"\n",
        "\n",
        "man_input_image = \"/content/drive/MyDrive/Interactive_cloth_segmentation_CV/input/fashionMENShortsid0000098901_1front.jpg\"\n",
        "man_output_image = \"/content/fashionMENShortsid0000098901_1front.jpg\"\n",
        "\n",
        "women_input_image = \"/content/drive/MyDrive/Interactive_cloth_segmentation_CV/input/fashionWOMENTees_Tanksid0000336104_1front.jpg\"\n",
        "women_output_image = \"/content/Output_Stylized_Image_women.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5ESChALKSe1"
      },
      "outputs": [],
      "source": [
        "!pip install flask-ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhMFh81_LDS3"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok==4.1.1\n",
        "!ngrok authtoken '2Ooqyfp3chBOMXWI4u58Pi7rCmk_4MJ6ho5vLENbTD9SnZrGU'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_Bj4OdmKbOJ"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, render_template, session, send_file, jsonify\n",
        "import os\n",
        "from werkzeug.utils import secure_filename\n",
        "\n",
        "from pyngrok import ngrok\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "# # Define allowed files\n",
        "ALLOWED_EXTENSIONS = {'txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif'}\n",
        "\n",
        "\n",
        "app = Flask(__name__, static_folder=\"/content/drive/MyDrive/ir_project/static\",template_folder=\"/content/drive/MyDrive/ir_project/templates\")\n",
        "app.config['UPLOAD_FOLDER'] = \"/content/drive/MyDrive/Interactive_cloth_segmentation_CV/input/\"\n",
        "app.secret_key = 'This is your secret key to utilize session in Flask'\n",
        "run_with_ngrok(app)  \n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template(\"home.html\")\n",
        "\n",
        "\n",
        "@app.route(\"/submit\", methods=[\"POST\"])\n",
        "def output():\n",
        "    img1 = request.files[\"image1\"]\n",
        "    img_filename = secure_filename(img1.filename)\n",
        "    img1.save(os.path.join(app.config['UPLOAD_FOLDER'], img_filename))\n",
        "    session['img1_file_path'] = os.path.join(app.config['UPLOAD_FOLDER'], img_filename)\n",
        "    img2 = request.files[\"image2\"]\n",
        "    img2_filename = secure_filename(img2.filename)\n",
        "    img2.save(os.path.join(app.config['UPLOAD_FOLDER'], img2_filename))\n",
        "    session['img2_file_path'] = os.path.join(app.config['UPLOAD_FOLDER'], img2_filename)\n",
        "    run_final(session['img1_file_path'], session['img2_file_path'], women_output_image, request.form('t_model'))\n",
        "    return render_template(\"output.html\", img1=\"Pose1.png\",img2=\"Pose2.png\", img3=\"Pose3.png\", img4=\"Pose4.png\")\n",
        "\n",
        "@app.route('/image/<filename>')\n",
        "def serve_image(filename):\n",
        "    path = \"/content/Results/Pose/\"+filename\n",
        "    print(path)\n",
        "    return send_file(path)\n",
        "\n",
        "@app.route(\"/getData\", methods=[\"GET\"])\n",
        "def data_to_colab():\n",
        "    images = [{'url': session[\"img1_file_path\"]}, {'url': session[\"img2_file_path\"]}]\n",
        "    return jsonify(images=images)\n",
        "\n",
        "\n",
        "app.run()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}